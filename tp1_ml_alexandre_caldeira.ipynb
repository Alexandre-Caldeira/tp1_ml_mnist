{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo deste notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TP1 ML - Alexandre Gomes Caldeira - 2024666544**\n",
    "\n",
    "Abaixo apresento minha solução ao Trabalho Prático 1 da disciplina DIP DCC831 PG3 - Tópicos Especiais em Ciência da Computação - Aprendizado de Máquina. Nessa seção é dado um resumo dos métodos e resultados obtidos como resposta aos requisitos do trabalho, cuja implementação pode ser vista em detalhes nas seções seguintes.\n",
    "\n",
    "Está disponível também um [Repositório Github](https://github.com/Alexandre-Caldeira/tp1_ml_mnist) ([github.com/Alexandre-Caldeira/tp1_ml_mnist](https://github.com/Alexandre-Caldeira/tp1_ml_mnist)) com histórico de implementação e resultados parciais. O repositório inclui requisitos para o ambiente de execução deste notebook para replicação dos resultados. Vale dizer que a execução dos resultados é demorada: 18hrs em CPU i7-8565U, 16 GB RAM.\n",
    "\n",
    "Método: \n",
    "- Aplicada validação cruzada com 5 *folds* (StratifiedKFold) em redes neuronais com regularização por decaimento de peso ajustado de maneira automatizada\n",
    "- Cada modelo teve taxa de aprendizado: 0.5, 1 e 10; neurônios na camada oculta: 25, 50 e 100; número de exemplos para cálculo de gradiente: 1 (SGD), 10 (mini-batch SGD), 50 (mini-batch SGD) e 3350 (todos os dados, GD);\n",
    "- O resultado de validação cruzada dos modelos regularizados é comparado, esperando-se observar e ilustrar o [\"*bias-variance trade-off*\"](https://arxiv.org/abs/1812.11118).\n",
    "\n",
    "Principais Resultados:\n",
    "1. Taxa de Aprendizado menor atinge performances melhores\n",
    "2. Mais exemplos para cada cálculo de gradiente resulta em performances melhores\n",
    "3. Mais neurônios na camada oculta não necessariamente melhora os resultados, parece haver um limite\n",
    "4. Mais neurônios na camada oculta performam melhor com taxas menores de aprendizado\n",
    "\n",
    "Disclaimer:\n",
    "- É válido aumentar o número de *folds*, variações de regularização e épocas para otimizar os resultados quantitativamente (nota final máxima de Acurácia e F1-score). Qualitativamente, nenhuma mudança será observada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pacotes utilizados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alguns pacotes (bibliotecas Python) foram utilizadas para:\n",
    "- **Implementação da rede neuronal, descida gradiente (estocástica)** ([PyTorch](https://pytorch.org/))\n",
    "- **Manipulação e visualização dos dados** ([Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [Matplotlib](https://matplotlib.org/))\n",
    "- **Separação de dados, implementação de métricas e validação cruzada** ([Scikit-learn](https://scikit-learn.org/))\n",
    "- **Otimização automatizada do hiperparametro de regularização L2** ([Optuna](https://optuna.org/))\n",
    "- **Registro de resultados e parâmetros de cada modelo durante e ao fim do treino** ([MLflow](https://mlflow.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sumário de resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Método**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. usando uma rede com 784 neurônios na camada de entrada (uma para pixel da imagem de entrada) e 10 neurônios na camada de saída, foram treinadas redes com 25, 50 e 100 neurônios na camada oculta com ativação sigmóide. \n",
    "2. O treino se deu por descida gradiente (clássica, estocástica e mini-batch com batches de 10 e 50 exemplos), usando como função de custo a entropia cruzada, utilizando taxa de aprendizado 0.5, 1 e 10. \n",
    "3. Foi aplicada regularização por decaimento de pesos (teoria: [Weight Decay](https://paperswithcode.com/method/weight-decay), implementação: [PyTorch Stochastic Gradient Descent with Weight Decay](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) cujo peso $\\lambda$ foi otimizado automaticamente com Optuna em 5 tentativas.\n",
    "4. O treino foi conduzido empregando validação cruzada de modelos treinados independentemente em 5 amostras aleatórias diferentes do dataset original, separando 67% dos dados para treino (3350 exemplos) e 33% dos dados para teste (1650 exemplos), utilizando a estratégia de [StratifiedKFold (Scikit-learn)](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) para k = 5 com embaralhamento (shuffling). \n",
    "5. Cada treino em cada *fold* teve seus parâmetros registrados via MLflow (lr - *taxa de aprendizado*, hidden_dim - *tamanho da camada oculta*, batch_size - *número de exemplos por batch de treino* e weight decay - *decaimento de pesos*), assim como as métricas durante o treinamento (valor da função de custo em treino e teste, [accuracy e F1-score](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)) e fora do treinamento (valor médio de acurácia e F1-score nos *folds*, certificado por validação cruzada).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Taxa de Aprendizado menor atinge performances melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/lr_cv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais exemplos para cada cálculo de gradiente resulta em performances melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/batch_size_cv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais neurônios na camada oculta não necessariamente melhora os resultados, parece haver um limite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](imgs/hidden_dim_cv.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais neurônios na camada oculta performam melhor com taxas menores de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/f1cv_hidden_dim_lr.jpg)\n",
    "\n",
    "![](imgs/acccv_hidden_dim_lr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Disclaimers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um número maior de épocas de treino e de tentativas (trials) de otimização de regularização Optuna poderiam aumentar a Acurácia e F1-score final dos modelos em geral, assim como um k maior de folds de validação cruzada poderia aumentar a confiabilidade do resultado final. Porém, devido à restrição de tempo (deadline de entrega do trabalho) optou-se por 50 épocas, 5 tentativas de otimização e 5 separações de dados para validação cruzada. Ainda assim, os resultados condizem com o esperado mediante teoria, e seguem as melhores práticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga e Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = pd.read_csv('data_tp1.csv',header=None)\n",
    "data = data.rename(columns={0:'y'})\n",
    "print(data.iloc[0,:].unique())\n",
    "\n",
    "imsize = round(np.sqrt(784)+1)\n",
    "linha_y = 4\n",
    "img = np.reshape(data.iloc[linha_y,1:].values, (imsize-1,imsize-1))\n",
    "plt.imshow(255-img,cmap='gray')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do formato do modelo e dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.y.values\n",
    "X = data.drop(columns='y').values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "idx = 0\n",
    "print((X_train[idx,:]).shape)\n",
    "print(y_train[idx])\n",
    "(y_train[idx]).shape \n",
    "\n",
    "# Pytorch exige que seja utilizado um dataset por algum motivo aparentemente\n",
    "class custom_mnist_dataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Restrições de uso do pytorch: (leia os docs...)\n",
    "        # inputs tem que ser float para multiplicar pelos pesos, \n",
    "        # outputs tem que ser longint para calcular crossentropy \n",
    "        return torch.tensor(self.X[idx,:], dtype=torch.float), self.y[idx]\n",
    "\n",
    "training_data = custom_mnist_dataset(X = X_train, y = y_train)\n",
    "test_data = custom_mnist_dataset(X = X_test, y = y_test)\n",
    "\n",
    "# batch_size de exemplo para teste\n",
    "batch_size = 25\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape, type: X: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape, type: y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "NeuralNetwork tem 3 camadas, com ativação sigmóide na camada oculta cujo tamanho será ajustado.\n",
    "\"\"\"\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.entrada = nn.Linear(input_size, hidden_dim)\n",
    "        self.oculta = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.saida = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x1 = self.entrada(input)\n",
    "        x2 = F.sigmoid(self.oculta(x1))\n",
    "        x3 = self.saida(x2)\n",
    "\n",
    "        return x3\n",
    "    \n",
    "# Exemplo hidden dim = 25\n",
    "print(NeuralNetwork(input_size = X_train.shape[1], hidden_dim= 25, output_size=10).to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos passos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referência utilizada e adaptada:\n",
    "# https://www.kaggle.com/code/shrutimechlearn/pytorch-custom-model-step-by-step\n",
    "\n",
    "def train_step(model, loss_func, optimizer, dataloader):\n",
    "    \n",
    "    # to capture loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # to get the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        # sending data to the device where rest of the artifacts are\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # forward pass/model prediction with the given input\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        # loss calculation by comparison between predicted and ground truth values\n",
    "        loss = loss_func(y_pred, y_batch)\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        # setting previously collected gradient values in the optimizer to zero so it translates only current gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate the gradients for this iteration (independent gradients because previous values have been reset to 0)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights and biases based on the calculated gradients ~(wi = wi + delta_wi)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def test_step(model, loss_func, test_dataloader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for batch, (x_batch, y_batch) in enumerate(test_dataloader):\n",
    "            \n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            test_loss+= loss.item()\n",
    "\n",
    "            _, preds = torch.max(y_pred, dim=1)\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss/len(test_dataloader)  \n",
    "    \n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "    precision = precision_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    recall = recall_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    f1 = f1_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "\n",
    "    return accuracy, precision, recall, f1, test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def train_and_evaluate_model(model, loss_func, optimizer, train_dataloader, val_dataloader, n_epochs, lr, hds, bs,weight_decay,i):   \n",
    "    with mlflow.start_run(run_name=f\"Test  #{i}\", nested = True):  # Name each run based on the learning rate\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)        \n",
    "\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "    \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            tr_loss = train_step(model, loss_func, optimizer, train_dataloader)\n",
    "\n",
    "            accuracy, precision, recall, f1, ts_loss = test_step(model, loss_func, test_dataloader)\n",
    "            \n",
    "            test_loss.append(ts_loss)   \n",
    "            train_losses.append(tr_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            mlflow.log_metric(\"loss\", tr_loss, step = epoch)\n",
    "            mlflow.log_metric(\"val_loss\", ts_loss, step = epoch)\n",
    "            mlflow.log_metric(\"diff_loss_tr_ts\", tr_loss-ts_loss, step = epoch)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy, step = epoch)\n",
    "            mlflow.log_metric(\"precision\", precision, step = epoch)\n",
    "            mlflow.log_metric(\"recall\", recall, step = epoch)\n",
    "            mlflow.log_metric(\"f1\", f1, step = epoch)  \n",
    "\n",
    "            \n",
    "            \n",
    "    return train_losses, val_losses, accuracies, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos testes automatizados e registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "def objective(trial, lr, hds, bs, X_train, y_train,n_epochs,n_splits,experiment_id):\n",
    "    \n",
    "    # Suggest regularization parameter using the trial object\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1)\n",
    "\n",
    "    text = \"Experiment - lr({lr}) hds({hds}) bs({bs}) wd({wd})\".format(lr =lr,hds=hds,bs =bs,wd = weight_decay)\n",
    "    with mlflow.start_run(run_name=text, experiment_id = experiment_id):\n",
    "        \n",
    "        # Initialize the model with fixed hyperparameters and suggested regularization\n",
    "        model = NeuralNetwork(input_size=X_train.shape[1], hidden_dim=hds, output_size=10).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Perform k-fold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        cv_scores = {\"accuracy\": [], \"f1\": []}\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, val_index in skf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            train_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_train_fold, y=y_train_fold), batch_size=bs)\n",
    "            val_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_val_fold, y=y_val_fold), batch_size=bs)\n",
    "\n",
    "            # Train and evaluate the model for this fold\n",
    "            train_loss, test_loss, accuracy, f1 = train_and_evaluate_model(\n",
    "                model, loss_func, optimizer, train_dataloader_fold, val_dataloader_fold, n_epochs=n_epochs, \n",
    "                lr=lr, hds=hds, bs=bs, weight_decay = weight_decay, i=fold)\n",
    "            \n",
    "            fold = fold+1\n",
    "\n",
    "            # Calculate average scores across all folds\n",
    "            cv_scores[\"accuracy\"].append(accuracy)\n",
    "            cv_scores[\"f1\"].append(f1)\n",
    "\n",
    "        # Return the average cross-validated scores\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)      \n",
    "        mlflow.log_metric(\"cv-accuracy\", np.mean(cv_scores[\"accuracy\"]))\n",
    "        mlflow.log_metric(\"cv-f1\",  np.mean(cv_scores[\"f1\"]))  \n",
    "                \n",
    "        return np.mean(cv_scores[\"accuracy\"]), np.mean(cv_scores[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução do Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LEMBRE-SE DE ATIVAR O SERVIDOR MLFLOW:**\n",
    "``` bash\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 5000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fixed hyperparameters\n",
    "fixed_hyperparams = {\n",
    "    'learning_rates': [0.5, 1, 10],\n",
    "    'hidden_dim_sizes': [25, 50, 100],\n",
    "    'batch_sizes': [1, 10, 50, len(training_data)],\n",
    "}\n",
    "\n",
    "# watch out for n_trials, n_splits, n_epochs\n",
    "n_trials = 5\n",
    "n_splits = 5\n",
    "n_epochs = 50\n",
    "\n",
    "# Set up MLflow tracking URI\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.autolog()\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Convert to string in the format 'YYYY-MM-DD HH:MM:SS'\n",
    "formatted_now = now.strftime('%Y%m%dT%H%M%S')\n",
    "print(formatted_now)\n",
    "\n",
    "mlflow_exp = mlflow.set_experiment(experiment_name  = 'TP1ML - '+formatted_now)\n",
    "\n",
    "# Iterate over fixed hyperparameters\n",
    "best_results = {}\n",
    "for lr in fixed_hyperparams['learning_rates']:\n",
    "    for hds in fixed_hyperparams['hidden_dim_sizes']:\n",
    "        for bs in fixed_hyperparams['batch_sizes']:\n",
    "            # Create an Optuna study for each combination\n",
    "            study_name =  \"Study - lr({lr}) hds({hds}) bs({bs}) - \".format(lr =lr,hds=hds,bs =bs)+formatted_now\n",
    "            study = optuna.create_study(directions=[\"minimize\", \"minimize\"],study_name =study_name )\n",
    "\n",
    "            study.optimize(\n",
    "                lambda t: objective(t, lr, hds, bs, X_train, y_train,n_epochs,n_splits,mlflow_exp.experiment_id)\n",
    "                , n_trials=n_trials\n",
    "                )\n",
    "            \n",
    "            # Record the best trial\n",
    "            best_trial = study.best_trials[0]\n",
    "            best_accuracy = best_trial.values[0]\n",
    "            best_f1 = best_trial.values[1]\n",
    "            \n",
    "            # Record best results\n",
    "            best_results[(lr, hds, bs)] = {\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_f1': best_f1,\n",
    "            }\n",
    "        \n",
    "        print(best_results[(lr, hds, bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the best results\n",
    "for (lr, hds, bs), result in best_results.items():\n",
    "    print(f\"Learning Rate: {lr}, Hidden Dimension Size: {hds}, Batch Size: {bs}\")\n",
    "    print(f\"Best Accuracy: {result['best_accuracy']}, Best F1 Score: {result['best_f1']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp1ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
