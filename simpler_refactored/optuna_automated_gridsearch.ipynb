{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7   0  84 185 159 151  60  36 222 254 241 198 170  52  67 114  72 163\n",
      " 227 225 250 229 140  17  66  14  59  21 236 106  83 253 209  18  22 233\n",
      " 255 129 238  44 249  62 133 187   5   9 205 248  58 126 182  75 251 240\n",
      "  57  19 221 166   3 203 219  35  38  77  31 224 115   1  61 242 121  40\n",
      " 207]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  781  782  \\\n",
       "0  7  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "1  2  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   783  784  \n",
       "0    0    0  \n",
       "1    0    0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data_tp1.csv',header=None)\n",
    "data = data.rename(columns={0:'y'})\n",
    "print(data.iloc[0,:].unique())\n",
    "\n",
    "imsize = round(np.sqrt(784)+1)\n",
    "linha_y = 4\n",
    "img = np.reshape(data.iloc[linha_y,1:].values, (imsize-1,imsize-1))\n",
    "plt.imshow(img)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def data and model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3350, 784)\n",
      "(3350,)\n",
      "(784,)\n",
      "7\n",
      "Shape, type: X: torch.Size([25, 784]) torch.float32\n",
      "Shape, type: y: torch.Size([25]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.y.values\n",
    "X = data.drop(columns='y').values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "idx = 0\n",
    "print((X_train[idx,:]).shape)\n",
    "print(y_train[idx])\n",
    "(y_train[idx]).shape \n",
    "\n",
    "# Pytorch exige que seja utilizado um dataset por algum motivo aparentemente\n",
    "class custom_mnist_dataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # RestriÃ§Ãµes de uso do pytorch: (leia os docs...)\n",
    "        # inputs tem que ser float para multiplicar pelos pesos, \n",
    "        # outputs tem que ser longint para calcular crossentropy \n",
    "        return torch.tensor(self.X[idx,:], dtype=torch.float), self.y[idx]\n",
    "\n",
    "training_data = custom_mnist_dataset(X = X_train, y = y_train)\n",
    "test_data = custom_mnist_dataset(X = X_test, y = y_test)\n",
    "\n",
    "# batch_size de exemplo para teste\n",
    "batch_size = 25\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape, type: X: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape, type: y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (entrada): Linear(in_features=784, out_features=25, bias=True)\n",
      "  (oculta): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (saida): Linear(in_features=25, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "NeuralNetwork with  ...\n",
    "\"\"\"\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.entrada = nn.Linear(input_size, hidden_dim)\n",
    "        self.oculta = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.saida = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x1 = self.entrada(input)\n",
    "        x2 = F.sigmoid(self.oculta(x1))\n",
    "        x3 = self.saida(x2)\n",
    "\n",
    "        return x3\n",
    "    \n",
    "# hidden dim = 25,50,100\n",
    "print(NeuralNetwork(input_size = X_train.shape[1], hidden_dim= 25, output_size=10).to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def train, test and eval method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/shrutimechlearn/pytorch-custom-model-step-by-step\n",
    "\n",
    "def train_step(model, loss_func, optimizer, dataloader):\n",
    "    \n",
    "    # to capture loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # to get the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        # sending data to the device where rest of the artifacts are\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # forward pass/model prediction with the given input\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        # loss calculation by comparison between predicted and ground truth values\n",
    "        loss = loss_func(y_pred, y_batch)\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        # setting previously collected gradient values in the optimizer to zero so it translates only current gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate the gradients for this iteration (independent gradients because previous values have been reset to 0)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights and biases based on the calculated gradients ~(wi = wi + delta_wi)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, loss_func, test_dataloader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for batch, (x_batch, y_batch) in enumerate(test_dataloader):\n",
    "            \n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            test_loss+= loss.item()\n",
    "\n",
    "    test_loss = test_loss/len(test_dataloader)   \n",
    "\n",
    "    return test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch import optim\n",
    "\n",
    "def eval_step_with_mlflowlogging(model,test_dataloader):\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_preds = []\n",
    "        total_labels = []\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "    precision = precision_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    recall = recall_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    f1 = f1_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def train_with_mlflowlogging(model, loss_func, optimizer, train_dataloader, test_dataloader, n_epochs, i,lr,hds,bs):\n",
    "    with mlflow.start_run(run_name=f\"Test #{i} - lr({lr}) hds({hds}) bs({bs})\", nested = True):  # Name each run based on the learning rate\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        \n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "    \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            tr_loss = train_step(model, loss_func, optimizer, train_dataloader)\n",
    "            train_loss.append(tr_loss)\n",
    "        \n",
    "            ts_loss = test_step(model, loss_func, test_dataloader)\n",
    "            test_loss.append(ts_loss)    \n",
    "\n",
    "            accuracy, precision, recall, f1 = eval_step_with_mlflowlogging(model, test_dataloader)\n",
    "            \n",
    "            mlflow.log_metric(\"loss\", tr_loss, step = epoch)\n",
    "            mlflow.log_metric(\"val_loss\", ts_loss, step = epoch)\n",
    "            mlflow.log_metric(\"diff_loss_tr_ts\", tr_loss-ts_loss, step = epoch)\n",
    "\n",
    "            mlflow.log_metric(\"loss\", tr_loss, step = epoch)\n",
    "            mlflow.log_metric(\"val_loss\", ts_loss, step = epoch)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy, step = epoch)\n",
    "            mlflow.log_metric(\"precision\", precision, step = epoch)\n",
    "            mlflow.log_metric(\"recall\", recall, step = epoch)\n",
    "            mlflow.log_metric(\"f1\", f1, step = epoch)   \n",
    "\n",
    "    return train_loss, test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def automated trials and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "# # Assuming the rest of your imports and setup code remains the same\n",
    "\n",
    "# def objective(trial, lr, hds, bs):\n",
    "#     # Suggest regularization parameter using the trial object\n",
    "#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
    "    \n",
    "#     # Initialize the model with fixed hyperparameters and suggested regularization\n",
    "#     model = NeuralNetwork(input_size=X_train.shape[1], hidden_dim=hds, output_size=10).to(device)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "#     # Training and evaluation steps remain the same, just ensure they use the model, loss_func, and optimizer initialized above\n",
    "#     # Note: You might need to adjust the train_with_mlflowlogging function to remove mlflow logging since we're focusing on Optuna integration now\n",
    "#     loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     # Example of how you might structure the training and evaluation inside the objective function\n",
    "#     for epoch in range(5):  # Number of epochs\n",
    "#         train_loss = train_step(model, loss_func, optimizer, train_dataloader)\n",
    "#         test_loss = test_step(model, loss_func, test_dataloader)\n",
    "#         # Evaluate other metrics as needed\n",
    "\n",
    "#     # Return a value that Optuna will try to minimize/maximize. For example, you could return the negative mean test loss\n",
    "#     return test_loss\n",
    "\n",
    "# # Fixed hyperparameters\n",
    "# fixed_hyperparams = {\n",
    "#     'learning_rates': [0.5, 1, 10],\n",
    "#     'hidden_dim_sizes': [25, 50, 100],\n",
    "#     'batch_sizes': [1, 10, 50, len(training_data)],\n",
    "# }\n",
    "\n",
    "\n",
    "# # Iterate over fixed hyperparameters\n",
    "# best_results = {}\n",
    "# for lr in fixed_hyperparams['learning_rates']:\n",
    "#     for hds in fixed_hyperparams['hidden_dim_sizes']:\n",
    "#         for bs in fixed_hyperparams['batch_sizes']:\n",
    "#             # Create an Optuna study for each combination\n",
    "#             study = optuna.create_study(direction='minimize')\n",
    "#             study.optimize(lambda t: objective(t, lr, hds, bs), n_trials=10)  # Adjust n_trials as needed\n",
    "            \n",
    "#             # Record the best trial\n",
    "#             best_trial = study.best_trial\n",
    "#             best_weight_decay = best_trial.params['weight_decay']\n",
    "#             best_test_loss = best_trial.value\n",
    "            \n",
    "#             # Record best results\n",
    "#             best_results[(lr, hds, bs)] = {\n",
    "#                 'weight_decay': best_weight_decay,\n",
    "#                 'test_loss': best_test_loss,\n",
    "#             }\n",
    "\n",
    "# # Print the best results\n",
    "# for (lr, hds, bs), result in best_results.items():\n",
    "#     print(f\"Learning Rate: {lr}, Hidden Dimension Size: {hds}, Batch Size: {bs}\")\n",
    "#     print(f\"Best Weight Decay: {result['weight_decay']}, Test Loss: {result['test_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/14 16:57:02 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2024/06/14 16:57:02 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/06/14 16:57:02 INFO mlflow.tracking.fluent: Experiment with name 'TP1ML - 20240614T165702' does not exist. Creating a new experiment.\n",
      "[I 2024-06-14 16:57:02,907] A new study created in memory with name: Study - lr(0.5) hds(25) bs(1) - 20240614T165702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240614T165702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 16:57:15,028] Trial 0 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.031512021818060454}. \n",
      "[I 2024-06-14 16:57:24,717] Trial 1 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.007073731890703579}. \n",
      "[I 2024-06-14 16:57:33,596] Trial 2 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.030003162307394464}. \n",
      "[I 2024-06-14 16:57:43,436] Trial 3 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.055299114247377476}. \n",
      "[I 2024-06-14 16:57:52,251] Trial 4 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.004109212093978322}. \n",
      "[I 2024-06-14 16:57:52,252] A new study created in memory with name: Study - lr(0.5) hds(25) bs(10) - 20240614T165702\n",
      "[I 2024-06-14 16:57:55,743] Trial 0 finished with values: [0.09523809523809523, 0.16666666666666666] and parameters: {'weight_decay': 5.122003098021528e-05}. \n",
      "[I 2024-06-14 16:57:59,322] Trial 1 finished with values: [0.047619047619047616, 0.08333333333333333] and parameters: {'weight_decay': 0.009538377212982964}. \n",
      "[I 2024-06-14 16:58:03,265] Trial 2 finished with values: [0.047619047619047616, 0.08333333333333333] and parameters: {'weight_decay': 0.012997443619474485}. \n",
      "[I 2024-06-14 16:58:07,942] Trial 3 finished with values: [0.09523809523809523, 0.16666666666666666] and parameters: {'weight_decay': 6.613954289931142e-05}. \n",
      "[I 2024-06-14 16:58:12,302] Trial 4 finished with values: [0.20634920634920637, 0.43333333333333335] and parameters: {'weight_decay': 0.0007921492960201513}. \n",
      "[I 2024-06-14 16:58:12,304] A new study created in memory with name: Study - lr(0.5) hds(25) bs(50) - 20240614T165702\n",
      "[I 2024-06-14 16:58:15,298] Trial 0 finished with values: [0.07965686274509803, 0.14642816190803806] and parameters: {'weight_decay': 0.00046007014326973644}. \n",
      "[I 2024-06-14 16:58:18,365] Trial 1 finished with values: [0.06004901960784314, 0.11328976034858389] and parameters: {'weight_decay': 0.00013564953544128322}. \n",
      "[I 2024-06-14 16:58:21,477] Trial 2 finished with values: [0.06004901960784314, 0.11328976034858389] and parameters: {'weight_decay': 0.0003982400185081152}. \n",
      "[I 2024-06-14 16:58:24,465] Trial 3 finished with values: [0.06004901960784314, 0.11328976034858389] and parameters: {'weight_decay': 0.0032229133836642197}. \n",
      "[I 2024-06-14 16:58:26,585] Trial 4 finished with values: [0.22058823529411764, 0.3792478354978355] and parameters: {'weight_decay': 3.4066258850082064e-05}. \n",
      "[I 2024-06-14 16:58:26,585] A new study created in memory with name: Study - lr(0.5) hds(25) bs(3350) - 20240614T165702\n",
      "[I 2024-06-14 16:58:28,486] Trial 0 finished with values: [0.30358508507063103, 0.27451992700212907] and parameters: {'weight_decay': 0.029560933491093053}. \n",
      "[I 2024-06-14 16:58:30,352] Trial 1 finished with values: [0.2021172195963544, 0.19245322960742647] and parameters: {'weight_decay': 0.018494847291466956}. \n",
      "[I 2024-06-14 16:58:32,301] Trial 2 finished with values: [0.16150745136796485, 0.18038209589798337] and parameters: {'weight_decay': 0.14737226835002243}. \n",
      "[I 2024-06-14 16:58:34,268] Trial 3 finished with values: [0.32392031908305335, 0.33090724384041725] and parameters: {'weight_decay': 0.03133298435606033}. \n",
      "[I 2024-06-14 16:58:36,304] Trial 4 finished with values: [0.2644807787543225, 0.2120661803605152] and parameters: {'weight_decay': 0.0008060636844647258}. \n",
      "[I 2024-06-14 16:58:36,305] A new study created in memory with name: Study - lr(0.5) hds(50) bs(1) - 20240614T165702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_accuracy': 0.16150745136796485, 'best_f1': 0.18038209589798337}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 16:58:49,264] Trial 0 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.009444547880974933}. \n",
      "[I 2024-06-14 16:59:03,475] Trial 1 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 7.029467075865969e-05}. \n",
      "[I 2024-06-14 16:59:21,360] Trial 2 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.0008101155269324142}. \n",
      "[I 2024-06-14 16:59:35,265] Trial 3 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.004247961408485963}. \n",
      "[I 2024-06-14 16:59:48,942] Trial 4 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.3196410120059011}. \n",
      "[I 2024-06-14 16:59:48,942] A new study created in memory with name: Study - lr(0.5) hds(50) bs(10) - 20240614T165702\n",
      "[I 2024-06-14 16:59:52,678] Trial 0 finished with values: [0.14285714285714285, 0.23148148148148148] and parameters: {'weight_decay': 1.5992903536527117e-05}. \n",
      "[I 2024-06-14 16:59:56,214] Trial 1 finished with values: [0.30952380952380953, 0.5092592592592592] and parameters: {'weight_decay': 0.000330878129572024}. \n",
      "[I 2024-06-14 16:59:59,535] Trial 2 finished with values: [0.14285714285714285, 0.23148148148148148] and parameters: {'weight_decay': 0.047133609839220075}. \n",
      "[I 2024-06-14 17:00:03,620] Trial 3 finished with values: [0.25396825396825395, 0.39814814814814814] and parameters: {'weight_decay': 8.758730054615093e-05}. \n",
      "[I 2024-06-14 17:00:12,418] Trial 4 finished with values: [0.25396825396825395, 0.39814814814814814] and parameters: {'weight_decay': 0.00014975326379632146}. \n",
      "[I 2024-06-14 17:00:12,427] A new study created in memory with name: Study - lr(0.5) hds(50) bs(50) - 20240614T165702\n",
      "[I 2024-06-14 17:00:17,714] Trial 0 finished with values: [0.27818627450980393, 0.35032679738562084] and parameters: {'weight_decay': 0.0034510510973479462}. \n",
      "[I 2024-06-14 17:00:22,914] Trial 1 finished with values: [0.27941176470588236, 0.508395061728395] and parameters: {'weight_decay': 0.12811453194688976}. \n",
      "[I 2024-06-14 17:00:28,238] Trial 2 finished with values: [0.20098039215686272, 0.34827160493827164] and parameters: {'weight_decay': 0.0037931178045199584}. \n",
      "[I 2024-06-14 17:00:33,180] Trial 3 finished with values: [0.2022058823529412, 0.45724489795918366] and parameters: {'weight_decay': 0.00011638872850759843}. \n",
      "[I 2024-06-14 17:00:38,907] Trial 4 finished with values: [0.07965686274509803, 0.14642816190803806] and parameters: {'weight_decay': 0.007173227057384348}. \n",
      "[I 2024-06-14 17:00:38,912] A new study created in memory with name: Study - lr(0.5) hds(50) bs(3350) - 20240614T165702\n",
      "[I 2024-06-14 17:00:42,081] Trial 0 finished with values: [0.3585218770623224, 0.3612560188157014] and parameters: {'weight_decay': 0.21368952590924747}. \n",
      "[I 2024-06-14 17:00:45,258] Trial 1 finished with values: [0.3203106866938559, 0.277104243157344] and parameters: {'weight_decay': 0.0018882319731563266}. \n",
      "[I 2024-06-14 17:00:48,662] Trial 2 finished with values: [0.34177729004020624, 0.31798983106337136] and parameters: {'weight_decay': 8.438609214054412e-05}. \n",
      "[I 2024-06-14 17:00:51,643] Trial 3 finished with values: [0.31940580514670097, 0.3126708733981411] and parameters: {'weight_decay': 0.0008115869208058121}. \n",
      "[I 2024-06-14 17:00:54,460] Trial 4 finished with values: [0.2758369886911199, 0.24092972953224] and parameters: {'weight_decay': 0.0023583243570197967}. \n",
      "[I 2024-06-14 17:00:54,460] A new study created in memory with name: Study - lr(0.5) hds(100) bs(1) - 20240614T165702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_accuracy': 0.2758369886911199, 'best_f1': 0.24092972953224}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 17:01:18,529] Trial 0 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.025155481466680174}. \n",
      "[I 2024-06-14 17:01:39,127] Trial 1 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.02049419223904946}. \n",
      "[I 2024-06-14 17:01:58,475] Trial 2 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.0003003672176996187}. \n",
      "[I 2024-06-14 17:02:16,292] Trial 3 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.1913457181349152}. \n",
      "[I 2024-06-14 17:02:31,965] Trial 4 finished with values: [0.3333333333333333, 0.3333333333333333] and parameters: {'weight_decay': 3.0259379292849047e-05}. \n",
      "[I 2024-06-14 17:02:31,965] A new study created in memory with name: Study - lr(0.5) hds(100) bs(10) - 20240614T165702\n",
      "[I 2024-06-14 17:02:35,711] Trial 0 finished with values: [0.1984126984126984, 0.4219576719576719] and parameters: {'weight_decay': 0.004578713147235193}. \n",
      "[I 2024-06-14 17:02:39,556] Trial 1 finished with values: [0.25396825396825395, 0.39814814814814814] and parameters: {'weight_decay': 0.0047714785481693485}. \n",
      "[I 2024-06-14 17:02:43,389] Trial 2 finished with values: [0.25396825396825395, 0.39814814814814814] and parameters: {'weight_decay': 4.5648118646548634e-05}. \n",
      "[I 2024-06-14 17:02:47,408] Trial 3 finished with values: [0.14285714285714285, 0.23148148148148148] and parameters: {'weight_decay': 0.0029632042030839877}. \n",
      "[I 2024-06-14 17:02:51,145] Trial 4 finished with values: [0.14285714285714285, 0.23148148148148148] and parameters: {'weight_decay': 0.023666953732340044}. \n",
      "[I 2024-06-14 17:02:51,161] A new study created in memory with name: Study - lr(0.5) hds(100) bs(50) - 20240614T165702\n",
      "[I 2024-06-14 17:02:54,265] Trial 0 finished with values: [0.18504901960784315, 0.2946078431372549] and parameters: {'weight_decay': 0.005436287822590278}. \n",
      "[I 2024-06-14 17:02:57,680] Trial 1 finished with values: [0.3799019607843137, 0.557685524352191] and parameters: {'weight_decay': 0.027725339160978916}. \n",
      "[I 2024-06-14 17:03:00,457] Trial 2 finished with values: [0.10171568627450982, 0.18556005398110662] and parameters: {'weight_decay': 0.10031474310128338}. \n",
      "[I 2024-06-14 17:03:03,577] Trial 3 finished with values: [0.32230392156862747, 0.6380864197530864] and parameters: {'weight_decay': 0.000888604435005199}. \n",
      "[I 2024-06-14 17:03:07,524] Trial 4 finished with values: [0.1213235294117647, 0.25925925925925924] and parameters: {'weight_decay': 0.13518119494761285}. \n",
      "[I 2024-06-14 17:03:07,524] A new study created in memory with name: Study - lr(0.5) hds(100) bs(3350) - 20240614T165702\n",
      "[I 2024-06-14 17:03:10,715] Trial 0 finished with values: [0.329247996371917, 0.35883895353928436] and parameters: {'weight_decay': 0.19129335391025126}. \n",
      "[I 2024-06-14 17:03:14,044] Trial 1 finished with values: [0.25343127659961345, 0.2842855776318781] and parameters: {'weight_decay': 0.0001572897239755264}. \n",
      "[I 2024-06-14 17:03:17,472] Trial 2 finished with values: [0.26117892374715085, 0.23419285023027903] and parameters: {'weight_decay': 0.0007533932108838836}. \n",
      "[I 2024-06-14 17:03:21,028] Trial 3 finished with values: [0.25194667188631437, 0.26161285853083754] and parameters: {'weight_decay': 5.409067029572417e-05}. \n",
      "[I 2024-06-14 17:03:24,169] Trial 4 finished with values: [0.30298664390558, 0.33421970479717483] and parameters: {'weight_decay': 0.0002399956802859154}. \n",
      "[I 2024-06-14 17:03:24,171] A new study created in memory with name: Study - lr(1) hds(25) bs(1) - 20240614T165702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_accuracy': 0.26117892374715085, 'best_f1': 0.23419285023027903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 17:03:35,656] Trial 0 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.023280851552434718}. \n",
      "[I 2024-06-14 17:03:45,568] Trial 1 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.023394649483790768}. \n",
      "[I 2024-06-14 17:03:55,076] Trial 2 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.000275765172172275}. \n",
      "[I 2024-06-14 17:04:06,267] Trial 3 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.00013304426215340692}. \n",
      "[I 2024-06-14 17:04:19,160] Trial 4 finished with values: [0.0, 0.0] and parameters: {'weight_decay': 0.07745971128588036}. \n",
      "[I 2024-06-14 17:04:19,160] A new study created in memory with name: Study - lr(1) hds(25) bs(10) - 20240614T165702\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Assuming the rest of your imports and setup code remains the same\n",
    "def train_and_evaluate_model(model, loss_func, optimizer, train_dataloader, val_dataloader, n_epochs, lr, hds, bs,weight_decay):   \n",
    "    with mlflow.start_run(run_name=f\"Testi \", nested = True):  # Name each run based on the learning rate\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)        \n",
    "\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Training loop\n",
    "            for inputs, targets in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_func(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            training_loss = loss\n",
    "            mlflow.log_metric(\"loss\", training_loss, step = epoch)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                total_targets = 0\n",
    "                total_outputs = 0\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, targets)\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    total_targets += targets.size(0)\n",
    "                    total_outputs += torch.sum(preds == targets.data)\n",
    "\n",
    "            total_labels = targets.numpy()\n",
    "            total_preds = preds.numpy()\n",
    "            acc = accuracy_score(total_labels, total_preds)\n",
    "            # precision = precision_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "            # recall = recall_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "            f1 = f1_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "\n",
    "            # acc = total_outputs / total_targets\n",
    "            # f1 = f1_score(targets.numpy(), preds.numpy())  # Assuming targets and preds are NumPy arrays\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            accuracies.append(acc)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            mlflow.log_metric(\"val_loss\", loss, step = epoch)\n",
    "            mlflow.log_metric(\"diff_loss_tr_ts\", training_loss-loss, step = epoch)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", acc, step = epoch)\n",
    "            # mlflow.log_metric(\"precision\", precision, step = epoch)\n",
    "            # mlflow.log_metric(\"recall\", recall, step = epoch)\n",
    "            mlflow.log_metric(\"f1\", f1, step = epoch)  \n",
    "\n",
    "        # return train_losses, val_losses, accuracies, f1_scores\n",
    "    return train_losses, val_losses, accuracies, f1_scores\n",
    "\n",
    "\n",
    "def objective(trial, lr, hds, bs, X_train, y_train):\n",
    "    text = \"Experiment - lr({lr}) hds({hds}) bs({bs})\".format(lr =lr,hds=hds,bs =bs)\n",
    "    with mlflow.start_run(run_name=text, experiment_id = mlflow_exp.experiment_id):\n",
    "        \n",
    "        # Suggest regularization parameter using the trial object\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-5, 0.5,log=True)\n",
    "        \n",
    "        # Initialize the model with fixed hyperparameters and suggested regularization\n",
    "        model = NeuralNetwork(input_size=X_train.shape[1], hidden_dim=hds, output_size=10).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Perform k-fold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        cv_scores = {\"accuracy\": [], \"f1\": []}\n",
    "\n",
    "        for train_index, val_index in skf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            train_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_train_fold, y=y_train_fold), batch_size=bs)\n",
    "            val_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_val_fold, y=y_val_fold), batch_size=bs)\n",
    "\n",
    "            # Train and evaluate the model for this fold\n",
    "            train_loss, test_loss, accuracy, f1 = train_and_evaluate_model(\n",
    "                model, loss_func, optimizer, train_dataloader_fold, val_dataloader_fold, n_epochs=1, \n",
    "                lr=lr, hds=hds, bs=bs, weight_decay = weight_decay)\n",
    "\n",
    "            # Calculate average scores across all folds\n",
    "            cv_scores[\"accuracy\"].append(accuracy)\n",
    "            cv_scores[\"f1\"].append(f1)\n",
    "\n",
    "        # Return the average cross-validated scores\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)      \n",
    "        mlflow.log_metric(\"cv-accuracy\", np.mean(cv_scores[\"accuracy\"]))\n",
    "        mlflow.log_metric(\"cv-f1\",  np.mean(cv_scores[\"f1\"]))  \n",
    "                \n",
    "        return np.mean(cv_scores[\"accuracy\"]), np.mean(cv_scores[\"f1\"])\n",
    "\n",
    "# Fixed hyperparameters\n",
    "fixed_hyperparams = {\n",
    "    'learning_rates': [0.5, 1, 10],\n",
    "    'hidden_dim_sizes': [25, 50, 100],\n",
    "    'batch_sizes': [1, 10, 50, len(training_data)],\n",
    "}\n",
    "\n",
    "# watch out for n_trials, n_splits, n_epochs\n",
    "# Set up MLflow tracking URI\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.autolog()\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Convert to string in the format 'YYYY-MM-DD HH:MM:SS'\n",
    "formatted_now = now.strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "print(formatted_now)\n",
    "\n",
    "mlflow_exp = mlflow.set_experiment(experiment_name  = 'TP1ML - '+formatted_now)\n",
    "\n",
    "# Iterate over fixed hyperparameters\n",
    "best_results = {}\n",
    "for lr in fixed_hyperparams['learning_rates']:\n",
    "    for hds in fixed_hyperparams['hidden_dim_sizes']:\n",
    "        for bs in fixed_hyperparams['batch_sizes']:\n",
    "            # Create an Optuna study for each combination\n",
    "            # study = optuna.create_study(direction=('minimize', ('accuracy', 'f1')))\n",
    "            study_name =  \"Study - lr({lr}) hds({hds}) bs({bs}) - \".format(lr =lr,hds=hds,bs =bs)+formatted_now\n",
    "            study = optuna.create_study(directions=[\"minimize\", \"minimize\"],study_name =study_name )\n",
    "            study.optimize(lambda t: objective(t, lr, hds, bs, X_train, y_train), n_trials=5)  # Adjust n_trials as needed\n",
    "            \n",
    "            # Record the best trial\n",
    "            best_trial = study.best_trials[0]\n",
    "            best_accuracy = best_trial.values[0]\n",
    "            best_f1 = best_trial.values[1]\n",
    "            \n",
    "            # Record best results\n",
    "            best_results[(lr, hds, bs)] = {\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_f1': best_f1,\n",
    "            }\n",
    "        \n",
    "        print(best_results[(lr, hds, bs)])\n",
    "\n",
    "# Print the best results\n",
    "for (lr, hds, bs), result in best_results.items():\n",
    "    print(f\"Learning Rate: {lr}, Hidden Dimension Size: {hds}, Batch Size: {bs}\")\n",
    "    print(f\"Best Accuracy: {result['best_accuracy']}, Best F1 Score: {result['best_f1']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp1ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
