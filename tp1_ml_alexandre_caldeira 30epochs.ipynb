{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo deste notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TP1 ML - Alexandre Gomes Caldeira - 2024666544**\n",
    "\n",
    "Abaixo apresento minha solução ao Trabalho Prático 1 da disciplina DIP DCC831 PG3 - Tópicos Especiais em Ciência da Computação - Aprendizado de Máquina. Nessa seção é dado um resumo dos métodos e resultados obtidos como resposta aos requisitos do trabalho, cuja implementação pode ser vista em detalhes nas seções seguintes.\n",
    "\n",
    "Está disponível também um [Repositório Github](https://github.com/Alexandre-Caldeira/tp1_ml_mnist) ([github.com/Alexandre-Caldeira/tp1_ml_mnist](https://github.com/Alexandre-Caldeira/tp1_ml_mnist)) com histórico de implementação e resultados parciais. O repositório inclui requisitos para o ambiente de execução deste notebook para replicação dos resultados. Vale dizer que a execução dos resultados é demorada: 18hrs em CPU i7-8565U, 16 GB RAM.\n",
    "\n",
    "Método: \n",
    "- Aplicada validação cruzada com 5 *folds* (StratifiedKFold) em redes neuronais com regularização por decaimento de peso ajustado de maneira automatizada\n",
    "- Cada modelo teve taxa de aprendizado: 0.5, 1 e 10; neurônios na camada oculta: 25, 50 e 100; número de exemplos para cálculo de gradiente: 1 (SGD), 10 (mini-batch SGD), 50 (mini-batch SGD) e 3350 (todos os dados, GD);\n",
    "- O resultado de validação cruzada dos modelos regularizados é comparado, esperando-se observar e ilustrar o [\"*bias-variance trade-off*\"](https://arxiv.org/abs/1812.11118).\n",
    "\n",
    "Principais Resultados:\n",
    "1. Taxa de Aprendizado menor atinge performances melhores\n",
    "2. Mais exemplos para cada cálculo de gradiente resulta em performances melhores\n",
    "3. Mais neurônios na camada oculta não necessariamente melhora os resultados, parece haver um limite\n",
    "4. Mais neurônios na camada oculta performam melhor com taxas menores de aprendizado\n",
    "\n",
    "Disclaimer:\n",
    "- É válido aumentar o número de *folds*, variações de regularização e épocas para otimizar os resultados quantitativamente (nota final máxima de Acurácia e F1-score). Qualitativamente, nenhuma mudança será observada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pacotes utilizados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alguns pacotes (bibliotecas Python) foram utilizadas para:\n",
    "- **Implementação da rede neuronal, descida gradiente (estocástica)** ([PyTorch](https://pytorch.org/))\n",
    "- **Manipulação e visualização dos dados** ([Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [Matplotlib](https://matplotlib.org/))\n",
    "- **Separação de dados, implementação de métricas e validação cruzada** ([Scikit-learn](https://scikit-learn.org/))\n",
    "- **Otimização automatizada do hiperparametro de regularização L2** ([Optuna](https://optuna.org/))\n",
    "- **Registro de resultados e parâmetros de cada modelo durante e ao fim do treino** ([MLflow](https://mlflow.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sumário de resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Método**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. usando uma rede com 784 neurônios na camada de entrada (uma para pixel da imagem de entrada) e 10 neurônios na camada de saída, foram treinadas redes com 25, 50 e 100 neurônios na camada oculta com ativação sigmóide. \n",
    "2. O treino se deu por descida gradiente (clássica, estocástica e mini-batch com batches de 10 e 50 exemplos), usando como função de custo a entropia cruzada, utilizando taxa de aprendizado 0.5, 1 e 10. \n",
    "3. Foi aplicada regularização por decaimento de pesos (teoria: [Weight Decay](https://paperswithcode.com/method/weight-decay), implementação: [PyTorch Stochastic Gradient Descent with Weight Decay](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) cujo peso $\\lambda$ foi otimizado automaticamente com Optuna em 5 tentativas.\n",
    "4. O treino foi conduzido empregando validação cruzada de modelos treinados independentemente em 5 amostras aleatórias diferentes do dataset original, separando 67% dos dados para treino (3350 exemplos) e 33% dos dados para teste (1650 exemplos), utilizando a estratégia de [StratifiedKFold (Scikit-learn)](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) para k = 5 com embaralhamento (shuffling). \n",
    "5. Cada treino em cada *fold* teve seus parâmetros registrados via MLflow (lr - *taxa de aprendizado*, hidden_dim - *tamanho da camada oculta*, batch_size - *número de exemplos por batch de treino* e weight decay - *decaimento de pesos*), assim como as métricas durante o treinamento (valor da função de custo em treino e teste, [accuracy e F1-score](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)) e fora do treinamento (valor médio de acurácia e F1-score nos *folds*, certificado por validação cruzada).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Taxa de Aprendizado menor atinge performances melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/lr_cv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais exemplos para cada cálculo de gradiente resulta em performances melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/batch_size_cv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais neurônios na camada oculta não necessariamente melhora os resultados, parece haver um limite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](imgs/hidden_dim_cv.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principais Resultados**: Mais neurônios na camada oculta performam melhor com taxas menores de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/f1cv_hidden_dim_lr.jpg)\n",
    "\n",
    "![](imgs/acccv_hidden_dim_lr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Disclaimers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um número maior de épocas de treino e de tentativas (trials) de otimização de regularização Optuna poderiam aumentar a Acurácia e F1-score final dos modelos em geral, assim como um k maior de folds de validação cruzada poderia aumentar a confiabilidade do resultado final. Porém, devido à restrição de tempo (deadline de entrega do trabalho) optou-se por 50 épocas, 5 tentativas de otimização e 5 separações de dados para validação cruzada. Ainda assim, os resultados condizem com o esperado mediante teoria, e seguem as melhores práticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga e Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7   0  84 185 159 151  60  36 222 254 241 198 170  52  67 114  72 163\n",
      " 227 225 250 229 140  17  66  14  59  21 236 106  83 253 209  18  22 233\n",
      " 255 129 238  44 249  62 133 187   5   9 205 248  58 126 182  75 251 240\n",
      "  57  19 221 166   3 203 219  35  38  77  31 224 115   1  61 242 121  40\n",
      " 207]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  781  782  \\\n",
       "0  7  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "1  2  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   783  784  \n",
       "0    0    0  \n",
       "1    0    0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = pd.read_csv('data_tp1.csv',header=None)\n",
    "data = data.rename(columns={0:'y'})\n",
    "print(data.iloc[0,:].unique())\n",
    "\n",
    "imsize = round(np.sqrt(784)+1)\n",
    "linha_y = 4\n",
    "img = np.reshape(data.iloc[linha_y,1:].values, (imsize-1,imsize-1))\n",
    "plt.imshow(255-img,cmap='gray')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do formato do modelo e dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3350, 784)\n",
      "(3350,)\n",
      "(784,)\n",
      "7\n",
      "Shape, type: X: torch.Size([25, 784]) torch.float32\n",
      "Shape, type: y: torch.Size([25]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.y.values\n",
    "X = data.drop(columns='y').values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "idx = 0\n",
    "print((X_train[idx,:]).shape)\n",
    "print(y_train[idx])\n",
    "(y_train[idx]).shape \n",
    "\n",
    "# Pytorch exige que seja utilizado um dataset por algum motivo aparentemente\n",
    "class custom_mnist_dataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Restrições de uso do pytorch: (leia os docs...)\n",
    "        # inputs tem que ser float para multiplicar pelos pesos, \n",
    "        # outputs tem que ser longint para calcular crossentropy \n",
    "        return torch.tensor(self.X[idx,:], dtype=torch.float), self.y[idx]\n",
    "\n",
    "training_data = custom_mnist_dataset(X = X_train, y = y_train)\n",
    "test_data = custom_mnist_dataset(X = X_test, y = y_test)\n",
    "\n",
    "# batch_size de exemplo para teste\n",
    "batch_size = 25\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape, type: X: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape, type: y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (entrada): Linear(in_features=784, out_features=25, bias=True)\n",
      "  (oculta): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (saida): Linear(in_features=25, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "NeuralNetwork tem 3 camadas, com ativação sigmóide na camada oculta cujo tamanho será ajustado.\n",
    "\"\"\"\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.entrada = nn.Linear(input_size, hidden_dim)\n",
    "        self.oculta = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.saida = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x1 = self.entrada(input)\n",
    "        x2 = F.sigmoid(self.oculta(x1))\n",
    "        x3 = self.saida(x2)\n",
    "\n",
    "        return x3\n",
    "    \n",
    "# Exemplo hidden dim = 25\n",
    "print(NeuralNetwork(input_size = X_train.shape[1], hidden_dim= 25, output_size=10).to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos passos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referência utilizada e adaptada:\n",
    "# https://www.kaggle.com/code/shrutimechlearn/pytorch-custom-model-step-by-step\n",
    "\n",
    "def train_step(model, loss_func, optimizer, dataloader):\n",
    "    \n",
    "    # to capture loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # to get the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        # sending data to the device where rest of the artifacts are\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # forward pass/model prediction with the given input\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        # loss calculation by comparison between predicted and ground truth values\n",
    "        loss = loss_func(y_pred, y_batch)\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        # setting previously collected gradient values in the optimizer to zero so it translates only current gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate the gradients for this iteration (independent gradients because previous values have been reset to 0)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights and biases based on the calculated gradients ~(wi = wi + delta_wi)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def test_step(model, loss_func, test_dataloader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for batch, (x_batch, y_batch) in enumerate(test_dataloader):\n",
    "            \n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            test_loss+= loss.item()\n",
    "\n",
    "            _, preds = torch.max(y_pred, dim=1)\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss/len(test_dataloader)  \n",
    "    \n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "    precision = precision_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    recall = recall_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "    f1 = f1_score(total_labels, total_preds, average='weighted', labels=np.unique(total_preds))\n",
    "\n",
    "    return accuracy, precision, recall, f1, test_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def train_and_evaluate_model(model, loss_func, optimizer, train_dataloader, val_dataloader, n_epochs, lr, hds, bs,weight_decay,i):   \n",
    "    with mlflow.start_run(run_name=f\"Test  #{i}\", nested = True):  # Name each run based on the learning rate\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)        \n",
    "\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "    \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            tr_loss = train_step(model, loss_func, optimizer, train_dataloader)\n",
    "\n",
    "            accuracy, precision, recall, f1, ts_loss = test_step(model, loss_func, test_dataloader)\n",
    "            \n",
    "            test_loss.append(ts_loss)   \n",
    "            train_losses.append(tr_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            mlflow.log_metric(\"loss\", tr_loss, step = epoch)\n",
    "            mlflow.log_metric(\"val_loss\", ts_loss, step = epoch)\n",
    "            mlflow.log_metric(\"diff_loss_tr_ts\", tr_loss-ts_loss, step = epoch)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy, step = epoch)\n",
    "            mlflow.log_metric(\"precision\", precision, step = epoch)\n",
    "            mlflow.log_metric(\"recall\", recall, step = epoch)\n",
    "            mlflow.log_metric(\"f1\", f1, step = epoch)  \n",
    "\n",
    "            \n",
    "            \n",
    "    return train_losses, val_losses, accuracies, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos testes automatizados e registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "def objective(trial, lr, hds, bs, X_train, y_train,n_epochs,n_splits,experiment_id):\n",
    "    \n",
    "    # Suggest regularization parameter using the trial object\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1)\n",
    "\n",
    "    text = \"Experiment - lr({lr}) hds({hds}) bs({bs}) wd({wd})\".format(lr =lr,hds=hds,bs =bs,wd = weight_decay)\n",
    "    with mlflow.start_run(run_name=text, experiment_id = experiment_id):\n",
    "        \n",
    "        # Initialize the model with fixed hyperparameters and suggested regularization\n",
    "        model = NeuralNetwork(input_size=X_train.shape[1], hidden_dim=hds, output_size=10).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Perform k-fold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        cv_scores = {\"accuracy\": [], \"f1\": []}\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, val_index in skf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            train_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_train_fold, y=y_train_fold), batch_size=bs)\n",
    "            val_dataloader_fold = DataLoader(custom_mnist_dataset(X=X_val_fold, y=y_val_fold), batch_size=bs)\n",
    "\n",
    "            # Train and evaluate the model for this fold\n",
    "            train_loss, test_loss, accuracy, f1 = train_and_evaluate_model(\n",
    "                model, loss_func, optimizer, train_dataloader_fold, val_dataloader_fold, n_epochs=n_epochs, \n",
    "                lr=lr, hds=hds, bs=bs, weight_decay = weight_decay, i=fold)\n",
    "            \n",
    "            fold = fold+1\n",
    "\n",
    "            # Calculate average scores across all folds\n",
    "            cv_scores[\"accuracy\"].append(accuracy)\n",
    "            cv_scores[\"f1\"].append(f1)\n",
    "\n",
    "        # Return the average cross-validated scores\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", bs)\n",
    "        mlflow.log_param(\"hidden_dim\", hds)\n",
    "        mlflow.log_param(\"lr\", lr)      \n",
    "        mlflow.log_metric(\"cv-accuracy\", np.mean(cv_scores[\"accuracy\"]))\n",
    "        mlflow.log_metric(\"cv-f1\",  np.mean(cv_scores[\"f1\"]))  \n",
    "                \n",
    "        return np.mean(cv_scores[\"accuracy\"]), np.mean(cv_scores[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução do Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LEMBRE-SE DE ATIVAR O SERVIDOR MLFLOW:**\n",
    "``` bash\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 5000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/16 17:03:01 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2024/06/16 17:03:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240616T170303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/16 17:03:05 INFO mlflow.tracking.fluent: Experiment with name 'TP1ML - 20240616T170303' does not exist. Creating a new experiment.\n",
      "[I 2024-06-16 17:03:05,753] A new study created in memory with name: Study - lr(0.5) hds(25) bs(1) - 20240616T170303\n",
      "[I 2024-06-16 17:19:09,890] Trial 0 finished with values: [0.10098181818181819, 0.1752869854297402] and parameters: {'weight_decay': 0.24728085096342484}. \n",
      "[I 2024-06-16 17:35:54,349] Trial 1 finished with values: [0.10024727272727273, 0.17797269908132762] and parameters: {'weight_decay': 0.6268175206694369}. \n",
      "[I 2024-06-16 17:52:39,358] Trial 2 finished with values: [0.09993454545454546, 0.17629312044697248] and parameters: {'weight_decay': 0.8028644129698758}. \n",
      "[I 2024-06-16 18:09:01,788] Trial 3 finished with values: [0.10133333333333334, 0.17841564362066822] and parameters: {'weight_decay': 0.6548906048119745}. \n",
      "[I 2024-06-16 18:24:39,203] Trial 4 finished with values: [0.10033212121212122, 0.17893996994549768] and parameters: {'weight_decay': 0.691505927223554}. \n",
      "[I 2024-06-16 18:24:39,211] A new study created in memory with name: Study - lr(0.5) hds(25) bs(10) - 20240616T170303\n",
      "[I 2024-06-16 18:29:05,583] Trial 0 finished with values: [0.09927030303030304, 0.17204930046690373] and parameters: {'weight_decay': 0.35536511350048583}. \n",
      "[I 2024-06-16 18:33:40,907] Trial 1 finished with values: [0.11163636363636366, 0.20043172636154163] and parameters: {'weight_decay': 0.10632704586411056}. \n",
      "[I 2024-06-16 18:38:25,823] Trial 2 finished with values: [0.12630545454545455, 0.23773885293637476] and parameters: {'weight_decay': 0.022877522552087092}. \n",
      "[I 2024-06-16 18:43:46,015] Trial 3 finished with values: [0.10517575757575759, 0.18337263957053082] and parameters: {'weight_decay': 0.17420294568015324}. \n",
      "[I 2024-06-16 18:48:57,788] Trial 4 finished with values: [0.11254545454545456, 0.19841281821226475] and parameters: {'weight_decay': 0.13072567425986042}. \n",
      "[I 2024-06-16 18:48:57,790] A new study created in memory with name: Study - lr(0.5) hds(25) bs(50) - 20240616T170303\n",
      "[I 2024-06-16 18:51:47,368] Trial 0 finished with values: [0.10200484848484849, 0.17623513889673853] and parameters: {'weight_decay': 0.7957936591714126}. \n",
      "[I 2024-06-16 18:54:29,040] Trial 1 finished with values: [0.10277575757575758, 0.1713716253814622] and parameters: {'weight_decay': 0.6928013441196943}. \n",
      "[I 2024-06-16 18:57:07,463] Trial 2 finished with values: [0.10581333333333334, 0.18013508548461693] and parameters: {'weight_decay': 0.6252203881335627}. \n",
      "[I 2024-06-16 18:59:50,119] Trial 3 finished with values: [0.15605818181818182, 0.26032351633185374] and parameters: {'weight_decay': 0.24965873544872397}. \n",
      "[I 2024-06-16 19:02:48,463] Trial 4 finished with values: [0.10401454545454547, 0.1753653351392224] and parameters: {'weight_decay': 0.7140788754942197}. \n",
      "[I 2024-06-16 19:02:48,463] A new study created in memory with name: Study - lr(0.5) hds(25) bs(3350) - 20240616T170303\n",
      "[I 2024-06-16 19:05:07,416] Trial 0 finished with values: [0.7496169696969697, 0.7404314699124733] and parameters: {'weight_decay': 0.013893979184101636}. \n",
      "[I 2024-06-16 19:07:15,828] Trial 1 finished with values: [0.11915636363636364, 0.20377174146126703] and parameters: {'weight_decay': 0.6041639320856387}. \n",
      "[I 2024-06-16 19:09:26,946] Trial 2 finished with values: [0.11256000000000001, 0.1890198264889166] and parameters: {'weight_decay': 0.9070075598746179}. \n",
      "[I 2024-06-16 19:11:27,856] Trial 3 finished with values: [0.5269866666666667, 0.4951714079911136] and parameters: {'weight_decay': 0.07738359980136102}. \n",
      "[I 2024-06-16 19:13:25,954] Trial 4 finished with values: [0.15899151515151516, 0.24432423807502254] and parameters: {'weight_decay': 0.3894061351894598}. \n",
      "[I 2024-06-16 19:13:25,971] A new study created in memory with name: Study - lr(0.5) hds(50) bs(1) - 20240616T170303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_accuracy': 0.7496169696969697, 'best_f1': 0.7404314699124733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-16 19:36:01,592] Trial 0 finished with values: [0.09965575757575758, 0.1746261481803236] and parameters: {'weight_decay': 0.5950587830527174}. \n",
      "[I 2024-06-16 20:00:01,873] Trial 1 finished with values: [0.10024727272727273, 0.17630094376411246] and parameters: {'weight_decay': 0.2740195898683157}. \n",
      "[I 2024-06-16 20:23:16,764] Trial 2 finished with values: [0.10156121212121212, 0.18179023059908828] and parameters: {'weight_decay': 0.05963837096467665}. \n",
      "[I 2024-06-16 20:44:05,378] Trial 3 finished with values: [0.1004121212121212, 0.1759911716499823] and parameters: {'weight_decay': 0.40734509765287696}. \n",
      "[I 2024-06-16 21:07:52,925] Trial 4 finished with values: [0.09981575757575757, 0.17402515931196466] and parameters: {'weight_decay': 0.5261409601137276}. \n",
      "[I 2024-06-16 21:07:52,926] A new study created in memory with name: Study - lr(0.5) hds(50) bs(10) - 20240616T170303\n",
      "[I 2024-06-16 21:12:00,865] Trial 0 finished with values: [0.09863030303030305, 0.1693962109225601] and parameters: {'weight_decay': 0.9729896298360254}. \n",
      "[I 2024-06-16 21:20:15,394] Trial 1 finished with values: [0.09864242424242425, 0.17020582813172055] and parameters: {'weight_decay': 0.5092854188639486}. \n",
      "[I 2024-06-16 21:31:32,460] Trial 2 finished with values: [0.09815030303030303, 0.17067642931506444] and parameters: {'weight_decay': 0.5847043507783999}. \n",
      "[I 2024-06-16 21:37:23,525] Trial 3 finished with values: [0.09845575757575757, 0.17184689913687134] and parameters: {'weight_decay': 0.6760751322017076}. \n",
      "[I 2024-06-16 21:42:16,330] Trial 4 finished with values: [0.09840000000000002, 0.16766344791341042] and parameters: {'weight_decay': 0.941252759651129}. \n",
      "[I 2024-06-16 21:42:16,330] A new study created in memory with name: Study - lr(0.5) hds(50) bs(50) - 20240616T170303\n",
      "[I 2024-06-16 21:45:01,791] Trial 0 finished with values: [0.10194424242424242, 0.1756845745287335] and parameters: {'weight_decay': 0.918155139163247}. \n",
      "[I 2024-06-16 21:47:47,345] Trial 1 finished with values: [0.10550303030303032, 0.18105224202122827] and parameters: {'weight_decay': 0.41097496370828174}. \n",
      "[I 2024-06-16 21:50:24,567] Trial 2 finished with values: [0.10101575757575759, 0.16898680866800134] and parameters: {'weight_decay': 0.7776054428222057}. \n",
      "[I 2024-06-16 21:52:59,453] Trial 3 finished with values: [0.10119272727272728, 0.17266728035973353] and parameters: {'weight_decay': 0.8674477467292859}. \n",
      "[I 2024-06-16 21:55:40,241] Trial 4 finished with values: [0.10688484848484849, 0.18371202682110616] and parameters: {'weight_decay': 0.4788011106544683}. \n",
      "[I 2024-06-16 21:55:40,241] A new study created in memory with name: Study - lr(0.5) hds(50) bs(3350) - 20240616T170303\n",
      "[I 2024-06-16 21:57:45,363] Trial 0 finished with values: [0.11471757575757575, 0.1898706985500287] and parameters: {'weight_decay': 0.5703224713082216}. \n",
      "[I 2024-06-16 21:59:48,259] Trial 1 finished with values: [0.11650424242424243, 0.19912847274877044] and parameters: {'weight_decay': 0.996100911331291}. \n",
      "[I 2024-06-16 22:01:53,664] Trial 2 finished with values: [0.6373236363636364, 0.6137131863313846] and parameters: {'weight_decay': 0.04755933443621823}. \n",
      "[I 2024-06-16 22:03:58,427] Trial 3 finished with values: [0.38626181818181815, 0.3622476326026888] and parameters: {'weight_decay': 0.13378043407142992}. \n",
      "[I 2024-06-16 22:06:06,982] Trial 4 finished with values: [0.10910545454545455, 0.18422952506838472] and parameters: {'weight_decay': 0.9028873311508707}. \n",
      "[I 2024-06-16 22:06:06,982] A new study created in memory with name: Study - lr(0.5) hds(100) bs(1) - 20240616T170303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_accuracy': 0.6373236363636364, 'best_f1': 0.6137131863313846}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-16 22:22:53,223] Trial 0 failed with parameters: {'weight_decay': 0.41962253537725014} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_28656\\1786724420.py\", line 36, in <lambda>\n",
      "    lambda t: objective(t, lr, hds, bs, X_train, y_train,n_epochs,n_splits,mlflow_exp.experiment_id)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_28656\\886324720.py\", line 32, in objective\n",
      "    train_loss, test_loss, accuracy, f1 = train_and_evaluate_model(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_28656\\1617907216.py\", line 32, in train_and_evaluate_model\n",
      "    mlflow.log_metric(\"diff_loss_tr_ts\", tr_loss-ts_loss, step = epoch)\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 818, in log_metric\n",
      "    return MlflowClient().log_metric(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\tracking\\client.py\", line 1494, in log_metric\n",
      "    return self._tracking_client.log_metric(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 561, in log_metric\n",
      "    self.store.log_metric(run_id, metric)\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py\", line 403, in log_metric\n",
      "    self._call_endpoint(LogMetric, req_body)\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py\", line 81, in _call_endpoint\n",
      "    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py\", line 302, in call_endpoint\n",
      "    response = http_request(**call_kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py\", line 129, in http_request\n",
      "    return _get_http_response_with_retries(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\mlflow\\utils\\request_utils.py\", line 228, in _get_http_response_with_retries\n",
      "    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 793, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 537, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\site-packages\\urllib3\\connection.py\", line 466, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\http\\client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\http\\client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\http\\client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alexa\\miniconda3\\envs\\tp1ml\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-16 22:22:53,268] Trial 0 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fixed hyperparameters\n",
    "fixed_hyperparams = {\n",
    "    'learning_rates': [0.5, 1, 10],\n",
    "    'hidden_dim_sizes': [25, 50, 100],\n",
    "    'batch_sizes': [1, 10, 50, len(training_data)],\n",
    "}\n",
    "\n",
    "# watch out for n_trials, n_splits, n_epochs\n",
    "n_trials = 5\n",
    "n_splits = 5\n",
    "n_epochs = 50\n",
    "\n",
    "# Set up MLflow tracking URI\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.autolog()\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Convert to string in the format 'YYYY-MM-DD HH:MM:SS'\n",
    "formatted_now = now.strftime('%Y%m%dT%H%M%S')\n",
    "print(formatted_now)\n",
    "\n",
    "mlflow_exp = mlflow.set_experiment(experiment_name  = 'TP1ML - '+formatted_now)\n",
    "\n",
    "# Iterate over fixed hyperparameters\n",
    "best_results = {}\n",
    "for lr in fixed_hyperparams['learning_rates']:\n",
    "    for hds in fixed_hyperparams['hidden_dim_sizes']:\n",
    "        for bs in fixed_hyperparams['batch_sizes']:\n",
    "            # Create an Optuna study for each combination\n",
    "            study_name =  \"Study - lr({lr}) hds({hds}) bs({bs}) - \".format(lr =lr,hds=hds,bs =bs)+formatted_now\n",
    "            study = optuna.create_study(directions=[\"maximize\", \"maximize\"],study_name =study_name )\n",
    "\n",
    "            study.optimize(\n",
    "                lambda t: objective(t, lr, hds, bs, X_train, y_train,n_epochs,n_splits,mlflow_exp.experiment_id)\n",
    "                , n_trials=n_trials\n",
    "                )\n",
    "            \n",
    "            # Record the best trial\n",
    "            best_trial = study.best_trials[0]\n",
    "            best_accuracy = best_trial.values[0]\n",
    "            best_f1 = best_trial.values[1]\n",
    "            \n",
    "            # Record best results\n",
    "            best_results[(lr, hds, bs)] = {\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_f1': best_f1,\n",
    "            }\n",
    "        \n",
    "        print(best_results[(lr, hds, bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.5, Hidden Dimension Size: 25, Batch Size: 1\n",
      "Best Accuracy: 0.1011709090909091, Best F1 Score: 0.1762214275085511\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 25, Batch Size: 10\n",
      "Best Accuracy: 0.09852363636363637, Best F1 Score: 0.17232837783508537\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 25, Batch Size: 50\n",
      "Best Accuracy: 0.10311757575757577, Best F1 Score: 0.17187873971878806\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 25, Batch Size: 3350\n",
      "Best Accuracy: 0.14158060606060605, Best F1 Score: 0.21666443792451145\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 50, Batch Size: 1\n",
      "Best Accuracy: 0.09961939393939394, Best F1 Score: 0.1741169141877573\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 50, Batch Size: 10\n",
      "Best Accuracy: 0.09812121212121212, Best F1 Score: 0.16730506503147444\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 50, Batch Size: 50\n",
      "Best Accuracy: 0.10273696969696972, Best F1 Score: 0.17019021312556604\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 50, Batch Size: 3350\n",
      "Best Accuracy: 0.1250230303030303, Best F1 Score: 0.2015123660041185\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 100, Batch Size: 1\n",
      "Best Accuracy: 0.09900121212121211, Best F1 Score: 0.17158055819482804\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 100, Batch Size: 10\n",
      "Best Accuracy: 0.09858424242424244, Best F1 Score: 0.1726286857242852\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 100, Batch Size: 50\n",
      "Best Accuracy: 0.10151757575757576, Best F1 Score: 0.16989654084181002\n",
      "Learning Rate: 0.5, Hidden Dimension Size: 100, Batch Size: 3350\n",
      "Best Accuracy: 0.10695515151515152, Best F1 Score: 0.18569627364409316\n",
      "Learning Rate: 1, Hidden Dimension Size: 25, Batch Size: 1\n",
      "Best Accuracy: 0.0996921212121212, Best F1 Score: 0.17557886284764973\n",
      "Learning Rate: 1, Hidden Dimension Size: 25, Batch Size: 10\n",
      "Best Accuracy: 0.09869818181818184, Best F1 Score: 0.17449127389958569\n",
      "Learning Rate: 1, Hidden Dimension Size: 25, Batch Size: 50\n",
      "Best Accuracy: 0.09747393939393939, Best F1 Score: 0.16883482103413283\n",
      "Learning Rate: 1, Hidden Dimension Size: 25, Batch Size: 3350\n",
      "Best Accuracy: 0.09797818181818183, Best F1 Score: 0.17669127495423625\n",
      "Learning Rate: 1, Hidden Dimension Size: 50, Batch Size: 1\n",
      "Best Accuracy: 0.0995660606060606, Best F1 Score: 0.17808350690652855\n",
      "Learning Rate: 1, Hidden Dimension Size: 50, Batch Size: 10\n",
      "Best Accuracy: 0.0986448484848485, Best F1 Score: 0.17445715503870132\n",
      "Learning Rate: 1, Hidden Dimension Size: 50, Batch Size: 50\n",
      "Best Accuracy: 0.09692121212121213, Best F1 Score: 0.17080598207725758\n",
      "Learning Rate: 1, Hidden Dimension Size: 50, Batch Size: 3350\n",
      "Best Accuracy: 0.10360727272727273, Best F1 Score: 0.1817151413757989\n",
      "Learning Rate: 1, Hidden Dimension Size: 100, Batch Size: 1\n",
      "Best Accuracy: 0.09898666666666667, Best F1 Score: 0.1726074153582751\n",
      "Learning Rate: 1, Hidden Dimension Size: 100, Batch Size: 10\n",
      "Best Accuracy: 0.09875151515151515, Best F1 Score: 0.17547525617371318\n",
      "Learning Rate: 1, Hidden Dimension Size: 100, Batch Size: 50\n",
      "Best Accuracy: 0.09709575757575757, Best F1 Score: 0.1724217078612049\n",
      "Learning Rate: 1, Hidden Dimension Size: 100, Batch Size: 3350\n",
      "Best Accuracy: 0.09967757575757576, Best F1 Score: 0.1716685663765134\n",
      "Learning Rate: 10, Hidden Dimension Size: 25, Batch Size: 1\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 25, Batch Size: 10\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 25, Batch Size: 50\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 25, Batch Size: 3350\n",
      "Best Accuracy: 0.09944242424242423, Best F1 Score: 0.1503089284749659\n",
      "Learning Rate: 10, Hidden Dimension Size: 50, Batch Size: 1\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 50, Batch Size: 10\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 50, Batch Size: 50\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 50, Batch Size: 3350\n",
      "Best Accuracy: 0.08824727272727274, Best F1 Score: 0.16215218633016765\n",
      "Learning Rate: 10, Hidden Dimension Size: 100, Batch Size: 1\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 100, Batch Size: 10\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 100, Batch Size: 50\n",
      "Best Accuracy: 0.08727272727272728, Best F1 Score: 0.16053511705685616\n",
      "Learning Rate: 10, Hidden Dimension Size: 100, Batch Size: 3350\n",
      "Best Accuracy: 0.08776242424242425, Best F1 Score: 0.16110863227115246\n"
     ]
    }
   ],
   "source": [
    "# Print the best results\n",
    "for (lr, hds, bs), result in best_results.items():\n",
    "    print(f\"Learning Rate: {lr}, Hidden Dimension Size: {hds}, Batch Size: {bs}\")\n",
    "    print(f\"Best Accuracy: {result['best_accuracy']}, Best F1 Score: {result['best_f1']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp1ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
